{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn_L): LSTM(50, 256, num_layers=2)\n",
      "  (rnn_M): LSTM(1, 256, num_layers=2)\n",
      "  (fc_L): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc_M): Linear(in_features=256, out_features=128, bias=True)\n",
      ")\n",
      "Epoch: 1, Batch Idx: 0, Loss: 1.2042142152786255\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 1, Acc: 0.0, Train loss: 0.4014047384262085\n",
      "Epoch: 2, Batch Idx: 0, Loss: 0.1802227646112442\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 2, Acc: 0.11111111442248027, Train loss: 0.060074254870414734\n",
      "Epoch: 3, Batch Idx: 0, Loss: 0.004099488724023104\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 3, Acc: 0.11111111442248027, Train loss: 0.0013664962413410346\n",
      "Epoch: 4, Batch Idx: 0, Loss: 0.0142610352486372\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 4, Acc: 0.11111111442248027, Train loss: 0.0047536784162123995\n",
      "Epoch: 5, Batch Idx: 0, Loss: 0.004343835171312094\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 5, Acc: 0.11111111442248027, Train loss: 0.0014479450571040313\n",
      "Epoch: 6, Batch Idx: 0, Loss: 0.010761500336229801\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 6, Acc: 0.11111111442248027, Train loss: 0.003587166778743267\n",
      "Epoch: 7, Batch Idx: 0, Loss: 0.002976943738758564\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 7, Acc: 0.11111111442248027, Train loss: 0.000992314579586188\n",
      "Epoch: 8, Batch Idx: 0, Loss: 0.008118942379951477\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 8, Acc: 0.11111111442248027, Train loss: 0.002706314126650492\n",
      "Epoch: 9, Batch Idx: 0, Loss: 0.0024951279629021883\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 9, Acc: 0.11111111442248027, Train loss: 0.0008317093209673961\n",
      "Epoch: 10, Batch Idx: 0, Loss: 0.006523573771119118\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 10, Acc: 0.11111111442248027, Train loss: 0.0021745245903730392\n",
      "Epoch: 11, Batch Idx: 0, Loss: 0.0021468643099069595\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 11, Acc: 0.11111111442248027, Train loss: 0.0007156214366356531\n",
      "Epoch: 12, Batch Idx: 0, Loss: 0.005648814607411623\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 12, Acc: 0.11111111442248027, Train loss: 0.001882938202470541\n",
      "Epoch: 13, Batch Idx: 0, Loss: 0.0019478644244372845\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 13, Acc: 0.11111111442248027, Train loss: 0.0006492881414790949\n",
      "Epoch: 14, Batch Idx: 0, Loss: 0.005003656260669231\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 14, Acc: 0.11111111442248027, Train loss: 0.001667885420223077\n",
      "Epoch: 15, Batch Idx: 0, Loss: 0.0016716247191652656\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 15, Acc: 0.11111111442248027, Train loss: 0.0005572082397217552\n",
      "Epoch: 16, Batch Idx: 0, Loss: 0.004313505720347166\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 16, Acc: 0.11111111442248027, Train loss: 0.001437835240115722\n",
      "Epoch: 17, Batch Idx: 0, Loss: 0.0014769729459658265\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 17, Acc: 0.11111111442248027, Train loss: 0.0004923243153219422\n",
      "Epoch: 18, Batch Idx: 0, Loss: 0.003847493790090084\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 18, Acc: 0.11111111442248027, Train loss: 0.001282497930030028\n",
      "Epoch: 19, Batch Idx: 0, Loss: 0.0013002101331949234\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 19, Acc: 0.11111111442248027, Train loss: 0.0004334033777316411\n",
      "Epoch: 20, Batch Idx: 0, Loss: 0.0034475266002118587\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 20, Acc: 0.11111111442248027, Train loss: 0.0011491755334039528\n",
      "Epoch: 21, Batch Idx: 0, Loss: 0.0011521432315930724\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 21, Acc: 0.11111111442248027, Train loss: 0.00038404774386435747\n",
      "Epoch: 22, Batch Idx: 0, Loss: 0.003071869956329465\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 22, Acc: 0.11111111442248027, Train loss: 0.0010239566521098216\n",
      "Epoch: 23, Batch Idx: 0, Loss: 0.0010053482837975025\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 23, Acc: 0.11111111442248027, Train loss: 0.0003351160945991675\n",
      "Epoch: 24, Batch Idx: 0, Loss: 0.0026164224836975336\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 24, Acc: 0.11111111442248027, Train loss: 0.0008721408278991779\n",
      "Epoch: 25, Batch Idx: 0, Loss: 0.0008905141148716211\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 25, Acc: 0.11111111442248027, Train loss: 0.0002968380382905404\n",
      "Epoch: 26, Batch Idx: 0, Loss: 0.002314091194421053\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 26, Acc: 0.11111111442248027, Train loss: 0.0007713637314736843\n",
      "Epoch: 27, Batch Idx: 0, Loss: 0.0007980343652889132\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 27, Acc: 0.11111111442248027, Train loss: 0.0002660114550963044\n",
      "Epoch: 28, Batch Idx: 0, Loss: 0.0021655908785760403\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 28, Acc: 0.11111111442248027, Train loss: 0.0007218636261920134\n",
      "Epoch: 29, Batch Idx: 0, Loss: 0.000758302106987685\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 29, Acc: 0.11111111442248027, Train loss: 0.00025276736899589497\n",
      "Epoch: 30, Batch Idx: 0, Loss: 0.0020052550826221704\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 30, Acc: 0.11111111442248027, Train loss: 0.0006684183608740568\n",
      "Epoch: 31, Batch Idx: 0, Loss: 0.0007080581272020936\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 31, Acc: 0.11111111442248027, Train loss: 0.0002360193757340312\n",
      "Epoch: 32, Batch Idx: 0, Loss: 0.0018772318726405501\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 32, Acc: 0.11111111442248027, Train loss: 0.0006257439575468501\n",
      "Epoch: 33, Batch Idx: 0, Loss: 0.0006834042142145336\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 33, Acc: 0.11111111442248027, Train loss: 0.00022780140473817786\n",
      "Epoch: 34, Batch Idx: 0, Loss: 0.0018025246681645513\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 34, Acc: 0.11111111442248027, Train loss: 0.0006008415560548505\n",
      "Epoch: 35, Batch Idx: 0, Loss: 0.0006698319339193404\n",
      "Time: 0 mins 1 secs\n",
      "Epoch: 35, Acc: 0.11111111442248027, Train loss: 0.0002232773113064468\n",
      "Epoch: 36, Batch Idx: 0, Loss: 0.0017426705453544855\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 36, Acc: 0.11111111442248027, Train loss: 0.0005808901817848285\n",
      "Epoch: 37, Batch Idx: 0, Loss: 0.0006690917070955038\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 37, Acc: 0.11111111442248027, Train loss: 0.0002230305690318346\n",
      "Epoch: 38, Batch Idx: 0, Loss: 0.0017009569564834237\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 38, Acc: 0.11111111442248027, Train loss: 0.0005669856521611413\n",
      "Epoch: 39, Batch Idx: 0, Loss: 0.0006541376351378858\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 39, Acc: 0.11111111442248027, Train loss: 0.00021804587837929526\n",
      "Epoch: 40, Batch Idx: 0, Loss: 0.0016731068026274443\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 40, Acc: 0.11111111442248027, Train loss: 0.0005577022675424814\n",
      "Epoch: 41, Batch Idx: 0, Loss: 0.0006366877933032811\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 41, Acc: 0.11111111442248027, Train loss: 0.00021222926443442702\n",
      "Epoch: 42, Batch Idx: 0, Loss: 0.0016344217583537102\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 42, Acc: 0.11111111442248027, Train loss: 0.0005448072527845701\n",
      "Epoch: 43, Batch Idx: 0, Loss: 0.0006335333455353975\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 43, Acc: 0.11111111442248027, Train loss: 0.0002111777818451325\n",
      "Epoch: 44, Batch Idx: 0, Loss: 0.001576168229803443\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 44, Acc: 0.11111111442248027, Train loss: 0.000525389409934481\n",
      "Epoch: 45, Batch Idx: 0, Loss: 0.0006064582848921418\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 45, Acc: 0.11111111442248027, Train loss: 0.00020215276163071394\n",
      "Epoch: 46, Batch Idx: 0, Loss: 0.0015351222828030586\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 46, Acc: 0.11111111442248027, Train loss: 0.0005117074276010195\n",
      "Epoch: 47, Batch Idx: 0, Loss: 0.0006117422599345446\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 47, Acc: 0.11111111442248027, Train loss: 0.0002039140866448482\n",
      "Epoch: 48, Batch Idx: 0, Loss: 0.0015011554351076484\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 48, Acc: 0.11111111442248027, Train loss: 0.0005003851450358828\n",
      "Epoch: 49, Batch Idx: 0, Loss: 0.0006036433042027056\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 49, Acc: 0.11111111442248027, Train loss: 0.0002012144347342352\n",
      "Epoch: 50, Batch Idx: 0, Loss: 0.0014521190896630287\n",
      "Time: 0 mins 0 secs\n",
      "Epoch: 50, Acc: 0.11111111442248027, Train loss: 0.0004840396965543429\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "skip_header=True\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_dim_L, hid_dim_L, emb_dim_M, hid_dim_M):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn_L = nn.LSTM(input_size=emb_dim_L,\n",
    "                           hidden_size=hid_dim_L,\n",
    "                           num_layers=2,\n",
    "                           bidirectional=False)\n",
    "        \n",
    "        self.rnn_M = nn.LSTM(input_size=emb_dim_M,\n",
    "                           hidden_size=hid_dim_M,\n",
    "                           num_layers=2,\n",
    "                           bidirectional=False)\n",
    "        \n",
    "        self.fc_L = nn.Linear(hid_dim_L, 128)\n",
    "        self.fc_M = nn.Linear(hid_dim_M, 128)\n",
    "\n",
    "\n",
    "    def forward(self, lyrics, melody):\n",
    "        output_L, (hidden_L, cell) = self.rnn_L(lyrics)\n",
    "        output_M, (hidden_M, cell) = self.rnn_M(melody)\n",
    "        final_output_L = self.fc_L(hidden_L[-1])\n",
    "        final_output_M = self.fc_M(hidden_M[-1])\n",
    "    \n",
    "        return final_output_L, final_output_M\n",
    "\n",
    "\n",
    "\n",
    "model_LM = RNN(50, 256, 1, 256)\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds==y).float()\n",
    "    acc = correct.sum() / len(correct) \n",
    "    return acc\n",
    "print (model_LM)\n",
    "# tensor1.reshape((457,50,1)).shape\n",
    "LR = 1e-3\n",
    "\n",
    "#create random dataset\n",
    "train_data_combined = [[torch.randn(300, 1, 50),torch.randn(300, 1, 1),1],[torch.randn(300, 1, 50),torch.randn(300, 1, 1),0],[torch.randn(300, 1, 50),torch.randn(300, 1, 1),1]]\n",
    "dev_data_combined = [[torch.randn(300, 1, 50),torch.randn(300, 1, 1),1],[torch.randn(300, 1, 50),torch.randn(300, 1, 1),0],[torch.randn(300, 1, 50),torch.randn(300, 1, 1),1]]\n",
    "\n",
    "def loss_fn(feature1, feature2, label):\n",
    "    similarity = cos(feature1, feature2)\n",
    "    #print (similarity.shape, label, feature1.shape, feature2.shape)\n",
    "    loss = nn.MSELoss() #loss 可改\n",
    "    return loss(similarity, label)\n",
    "\n",
    "def train(model_LM, train_data_combined, epoch):\n",
    "\n",
    "    optimizer = optim.Adam(model_LM.parameters(), lr=LR) #list object has no attribute parameters后面的train也是一样\n",
    "    \n",
    "#     loss_fn = nn.BCEWithLogitsLoss(loss, label)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model_LM.train()\n",
    "    for batch_idx, batch in enumerate(train_data_combined):\n",
    "        lyrics = batch[0]#.lyrics\n",
    "        note = batch[1]#.note\n",
    "        label = batch[2]#.label\n",
    "        label = torch.autograd.variable(label).float()\n",
    "        feature_L, feature_M = model_LM(lyrics, note)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = loss_fn(feature_L, feature_M, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print (\"Epoch: {}, Batch Idx: {}, Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "        return epoch_loss/len(train_data_combined)\n",
    "    \n",
    "    \n",
    "\n",
    "def evaluate(model_LM, dev_data_combined):\n",
    "\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #model_LM.test()\n",
    "    for batch_idx, batch in enumerate(dev_data_combined):\n",
    "        lyrics = batch[0]#.lyrics\n",
    "        note = batch[1]#.note\n",
    "        label = batch[2]#.label\n",
    "        \n",
    "        feature_L, feature_M = model_LM(lyrics, note) #text_lengths).squeeze(1)\n",
    "        \n",
    "        pred = (cos(feature_L, feature_M) > 0.6)\n",
    "        \n",
    "        acc = sum(pred == label)*1.0/len(dev_data_combined)\n",
    "        \n",
    "        epoch_acc += acc.item()\n",
    "        return epoch_acc/len(dev_data_combined)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        mins = int(elapsed_time / 60)\n",
    "        secs = int(elapsed_time - (mins)*60)\n",
    "        print (\"Time: {} mins {} secs\".format(mins, secs))\n",
    "\n",
    "EPOCH = 50\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "for epoch in range(1, EPOCH+1):\n",
    "        start_time = time.time() \n",
    "    \n",
    "        train_loss = train(model_LM, train_data_combined, epoch)\n",
    "        \n",
    "        dev_acc = evaluate(model_LM, dev_data_combined)\n",
    "        end_time = time.time()\n",
    "        epoch_time(start_time, end_time)\n",
    "        print (\"Epoch: {}, Acc: {}, Train loss: {}\".format(epoch, dev_acc, train_loss))\n",
    "\n",
    "# save model\n",
    "SAVE_PATH = \"matcher_rnn.ckpt\"\n",
    "torch.save(model_LM.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model \n",
    "# model = RNN(emb_dim, hid_dim)\n",
    "# model.load_state_dict(torch.load(SAVE_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
